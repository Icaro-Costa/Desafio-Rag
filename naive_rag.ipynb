import os
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
# Importe seu LLM (ex: Groq, OpenAI, ou um local via HuggingFace)
# from langchain_groq import ChatGroq 

# --- CONFIGURAÇÃO ---
# Substitua pelo caminho do seu PDF
PDF_PATH = "caminho/para/os_sertoes.pdf" 
# Modelo de embedding open-source
EMBEDDING_MODEL = "BAAI/bge-m3" 
# Configure seu LLM
# llm = ChatGroq(model_name="llama3-8b-8192", temperature=0) 

# --- 1. CARREGAR E DIVIDIR (Load & Split) ---
loader = PyPDFLoader(PDF_PATH)
docs = loader.load()

# Divide o texto em chunks
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000, 
    chunk_overlap=200
)
splits = text_splitter.split_documents(docs)

# --- 2. EMBUTIR E ARMAZENAR (Embed & Store) ---
print("Criando embeddings e vetorizando...")
embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)

# Cria o vector store com os chunks
vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)

# Expõe o vector store como um retriever
retriever = vectorstore.as_retriever(search_kwargs={"k": 5}) # 'k' é o número de chunks a recuperar

# --- 3. DEFINIR PROMPT E CADEIA (Prompt & Chain) ---
prompt_template = """
Use o contexto fornecido para responder à pergunta.
Se você não sabe a resposta, diga "Não sei".

Contexto:
{context}

Pergunta:
{input}

Resposta:
"""
prompt = ChatPromptTemplate.from_template(prompt_template)

# --- 4. CRIAR A CADEIA RAG (RAG Chain) ---

# Função para formatar os documentos recuperados
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# Define a cadeia (chain)
rag_chain_naive = (
    {"context": retriever | format_docs, "input": RunnablePassthrough()}
    | prompt
    # | llm  # Descomente quando o LLM estiver configurado
    # | StrOutputParser()
)

# --- 5. EXECUTAR AS PERGUNTAS ---
lista_perguntas = [
    "Qual é a visão de Euclides da Cunha sobre o ambiente natural do sertão nordestino e como ele influencia a vida dos habitantes?",
    "Quais são as principais características da população sertaneja descritas por Euclides da Cunha? Como ele relaciona essas características com o ambiente em que vivem?",
    # ... adicione as outras perguntas
]

print("\n--- Respostas do Naive RAG ---")
for pergunta in lista_perguntas:
    print(f"\nPergunta: {pergunta}")
    # resposta = rag_chain_naive.invoke(pergunta)
    # print(f"Resposta: {resposta}")
    print("Resposta: [LLM desativado neste exemplo]")
